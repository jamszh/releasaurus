"use strict";(self.webpackChunkreleasaurus=self.webpackChunkreleasaurus||[]).push([[835],{3905:(e,t,r)=>{r.d(t,{Zo:()=>c,kt:()=>d});var a=r(7294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function i(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function l(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?i(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function o(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},i=Object.keys(e);for(a=0;a<i.length;a++)r=i[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)r=i[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var s=a.createContext({}),u=function(e){var t=a.useContext(s),r=t;return e&&(r="function"==typeof e?e(t):l(l({},t),e)),r},c=function(e){var t=u(e.components);return a.createElement(s.Provider,{value:t},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,i=e.originalType,s=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),p=u(r),h=n,d=p["".concat(s,".").concat(h)]||p[h]||m[h]||i;return r?a.createElement(d,l(l({ref:t},c),{},{components:r})):a.createElement(d,l({ref:t},c))}));function d(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=r.length,l=new Array(i);l[0]=h;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[p]="string"==typeof e?e:n,l[1]=o;for(var u=2;u<i;u++)l[u]=r[u];return a.createElement.apply(null,l)}return a.createElement.apply(null,r)}h.displayName="MDXCreateElement"},3249:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>m,frontMatter:()=>i,metadata:()=>o,toc:()=>u});var a=r(7462),n=(r(7294),r(3905));const i={title:"v2.0.1-pytorch",date:new Date("2023-05-08T19:55:19.000Z"),tags:["pytorch"],authors:{name:"drisspg",url:"https://github.com/drisspg",image_url:"https://avatars.githubusercontent.com/u/32754868?v=4"}},l=void 0,o={permalink:"/releasaurus/releases/v2.0.1-pytorch",editUrl:"https://github.com/jamszh/releasaurus/tree/main/releases/v2.0.1-pytorch.mdx",source:"@site/releases/v2.0.1-pytorch.mdx",title:"v2.0.1-pytorch",description:"This release is meant to fix the following issues (regressions / silent correctness):",date:"2023-05-08T19:55:19.000Z",formattedDate:"May 8, 2023",tags:[{label:"pytorch",permalink:"/releasaurus/releases/tags/pytorch"}],readingTime:2.19,hasTruncateMarker:!1,authors:[{name:"drisspg",url:"https://github.com/drisspg",image_url:"https://avatars.githubusercontent.com/u/32754868?v=4",imageURL:"https://avatars.githubusercontent.com/u/32754868?v=4"}],frontMatter:{title:"v2.0.1-pytorch",date:"2023-05-08T19:55:19.000Z",tags:["pytorch"],authors:{name:"drisspg",url:"https://github.com/drisspg",image_url:"https://avatars.githubusercontent.com/u/32754868?v=4",imageURL:"https://avatars.githubusercontent.com/u/32754868?v=4"}}},s={authorsImageUrls:[void 0]},u=[{value:"Torch.compile:",id:"torchcompile",level:3},{value:"Distributed:",id:"distributed",level:3},{value:"NN_frontend:",id:"nn_frontend",level:3},{value:"DataLoader:",id:"dataloader",level:3},{value:"MPS:",id:"mps",level:3},{value:"Functorch:",id:"functorch",level:3},{value:"Releng:",id:"releng",level:3},{value:"Torch.optim:",id:"torchoptim",level:3}],c={toc:u},p="wrapper";function m(e){let{components:t,...r}=e;return(0,n.kt)(p,(0,a.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("p",null,"This release is meant to fix the following issues (regressions / silent correctness):"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Fix ",(0,n.kt)("inlineCode",{parentName:"li"},"_canonical_mask")," throws warning when bool masks passed as input to TransformerEncoder/TransformerDecoder (#96009, #96286) "),(0,n.kt)("li",{parentName:"ul"},"Fix Embedding bag max_norm=-1 causes leaf Variable that requires grad is being used in an in-place operation #95980"),(0,n.kt)("li",{parentName:"ul"},"Fix type hint for torch.Tensor.grad_fn, which can be a torch.autograd.graph.Node or None. #96804"),(0,n.kt)("li",{parentName:"ul"},"Can\u2019t convert float to int when the input is a scalar np.ndarray. #97696"),(0,n.kt)("li",{parentName:"ul"},"Revisit torch._six.string_classes removal  #97863"),(0,n.kt)("li",{parentName:"ul"},"Fix module backward pre-hooks to actually update gradient #97983"),(0,n.kt)("li",{parentName:"ul"},"Fix load_sharded_optimizer_state_dict error on multi node #98063"),(0,n.kt)("li",{parentName:"ul"},"Warn once for TypedStorage deprecation #98777"),(0,n.kt)("li",{parentName:"ul"},"cuDNN V8 API, Fix incorrect use of emplace in the benchmark cache #97838")),(0,n.kt)("h3",{id:"torchcompile"},"Torch.compile:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Add support for Modules with custom ",(0,n.kt)("strong",{parentName:"li"},"getitem")," method to torch.compile #97932"),(0,n.kt)("li",{parentName:"ul"},"Fix improper guards with on list variables. #97862"),(0,n.kt)("li",{parentName:"ul"},"Fix Sequential nn module with duplicated submodule #98880")),(0,n.kt)("h3",{id:"distributed"},"Distributed:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Fix distributed_c10d's handling of custom backends #95072"),(0,n.kt)("li",{parentName:"ul"},"Fix MPI backend not properly initialized #98545")),(0,n.kt)("h3",{id:"nn_frontend"},"NN_frontend:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Update Multi-Head Attention's doc string #97046"),(0,n.kt)("li",{parentName:"ul"},"Fix incorrect behavior of ",(0,n.kt)("inlineCode",{parentName:"li"},"is_causal")," paremeter for torch.nn.TransformerEncoderLayer.forward #97214"),(0,n.kt)("li",{parentName:"ul"},"Fix error for SDPA on sm86 and sm89 hardware #99105"),(0,n.kt)("li",{parentName:"ul"},"Fix nn.MultiheadAttention mask handling  #98375")),(0,n.kt)("h3",{id:"dataloader"},"DataLoader:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Fix regression for pin_memory recursion when operating on bytes #97737"),(0,n.kt)("li",{parentName:"ul"},"Fix collation logic #97789 "),(0,n.kt)("li",{parentName:"ul"},"Fix Ppotentially backwards incompatible change with DataLoader and is_shardable Datapipes #97287")),(0,n.kt)("h3",{id:"mps"},"MPS:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Fix LayerNorm crash when input is in float16 #96208"),(0,n.kt)("li",{parentName:"ul"},"Add support for cumsum on int64 input  #96733"),(0,n.kt)("li",{parentName:"ul"},"Fix issue with setting BatchNorm to non-trainable #98794")),(0,n.kt)("h3",{id:"functorch"},"Functorch:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Fix Segmentation Fault for vmaped function accessing BatchedTensor.data #97237"),(0,n.kt)("li",{parentName:"ul"},"Fix index_select support when dim is negative ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/pytorch/pytorch/pull/97916"},"#97916")),(0,n.kt)("li",{parentName:"ul"},"Improve docs for autograd.Function support #98020"),(0,n.kt)("li",{parentName:"ul"},"Fix Exception thrown when running Migration guide example for jacrev #97746")),(0,n.kt)("h3",{id:"releng"},"Releng:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Fix Convolutions for CUDA-11.8 wheel builds #99451"),(0,n.kt)("li",{parentName:"ul"},"Fix Import torchaudio + torch.compile crashes on exit #96231"),(0,n.kt)("li",{parentName:"ul"},"Linux aarch64 wheels are missing the mkldnn+acl backend support  - ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/pytorch/builder/commit/54931c264ed3e7346899f547a272c4329cc8933b"},"https://github.com/pytorch/builder/commit/54931c264ed3e7346899f547a272c4329cc8933b")),(0,n.kt)("li",{parentName:"ul"},"Linux aarch64 torchtext 0.15.1 wheels are missing for aarch64_linux platform - ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/pytorch/builder/issues/1375"},"https://github.com/pytorch/builder/issues/1375")),(0,n.kt)("li",{parentName:"ul"},"Enable ROCm 5.4.2 manywheel and python 3.11 builds #99552"),(0,n.kt)("li",{parentName:"ul"},"PyTorch cannot be installed at the same time as numpy in a conda env on osx-64 / Python 3.11 #97031"),(0,n.kt)("li",{parentName:"ul"},"Illegal instruction (core dumped) on Raspberry Pi 4.0 8gb  - ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/pytorch/builder/pull/1370"},"https://github.com/pytorch/builder/pull/1370"))),(0,n.kt)("h3",{id:"torchoptim"},"Torch.optim:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Fix fused AdamW causes NaN loss #95847"),(0,n.kt)("li",{parentName:"ul"},"Fix Fused AdamW has worse loss than Apex and unfused AdamW for fp16/AMP #98620")),(0,n.kt)("p",null,"The ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/pytorch/issues/97272"},"release tracker")," should contain all relevant pull requests related to this release as well as links to related issues"))}m.isMDXComponent=!0}}]);